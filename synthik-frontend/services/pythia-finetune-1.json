{
  "job_name": "Pythia-70M Sarcasm LoRA - test (HF)",
  "user_wallet_address": "0x37793860ea65a1e05a9a506ed7b86b084cb9bba5fc9c979da3512464007fa11d",
  "dataset_url": "https://0x311e26702aba231c321c633d1ff6ecb4445f2308.calibration.filcdn.io/baga6ea4seaqhv7zvx7ykx6pady5fk5fbz422ohupjgd6vvzwwjnlotgs22lzqka",
  "file_type": "csv",
  "platform": "hugging_face",
  "user_credential_id": "5b0a3997-d285-45e4-9542-132d92ad6a27",
  "model_type": "CAUSAL_LM",
  "hyperparameters": {
    "base_model_id": "EleutherAI/pythia-70m-deduped",
    "model_task_type": "CAUSAL_LM",
    "epochs": 1,
    "learning_rate": 0.0002,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 4,
    "max_seq_length": 512,
    "text_column": "text",
    "load_in_4bit": true,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "logging_steps": 10,
    "save_steps": 100
  },
  "training_script_config": {
    "training_script_name": "train_text_lora.py",
    "hf_username": "Testys",
    "hf_target_model_repo_id": "Testys/Eluether-finetune",
    "hf_space_hardware": "t4-small",
    "hf_private_repos": false,
    "model_output_dir_in_space": "/outputs",
    "report_to": "tensorboard"
  },
  "id": "ba87928a-c041-4ec0-9900-e65fc7d8afa6",
  "status": "pending",
  "external_job_id": null,
  "metrics": null,
  "output_model_storage_type": null,
  "output_model_url": null,
  "huggingface_model_url": null,
  "logs_url": null,
  "error_message": null,
  "created_at": "2025-07-06T17:54:23.791023Z",
  "updated_at": "2025-07-06T17:54:23.791023Z",
  "started_at": null,
  "completed_at": null
}
