# hf_uploader.py
import logging
import os
import tempfile
import aios
import asyncio
import shutil
import tarfile
import zipfile
import json # For model card data, config.json etc.
from pathlib import Path
from typing import Optional, Dict, Any, Union
from urllib.parse import urlparse
import aiofiles

# --- Cloud SDKs ---
import boto3 # For session and config, aiobotocore uses it.
import aiobotocore.session # For async S3
from google.cloud import storage # For GCS (sync client, use threadpool for async)
from google.oauth2 import service_account # For GCS creds

# --- Hugging Face Hub ---
from huggingface_hub import HfApi, create_repo, ModelCardData # ModelCard for structured metadata
from huggingface_hub.utils import HfHubHTTPError # For specific HF errors

# --- Application Specific Imports ---
from app.ai_training.models import AITrainingJob # Your SQLAlchemy model
from app.core.enums.ai_training import StorageType # Your Enum
# from app.core.config import settings # For MLOps credentials
from app.core.constants import (
    AWS_REGION_MLOPS,
    GCP_PROJECT_ID_MLOPS,
    GCP_SERVICE_ACCOUNT_KEY_PATH_MLOPS,
)

logger = logging.getLogger(__name__)
# Ensure logging is configured if running this standalone for testing
# logging.basicConfig(level=logging.INFO)


def _generate_model_card_content_from_job(job_details: AITrainingJob) -> str:
    """
    Generates Markdown model card content based on AITrainingJob details.
    """
    # Basic dataset info
    dataset_name = job_details.processed_dataset.name if job_details.processed_dataset else "N/A"
    dataset_storage_url = job_details.processed_dataset.storage_url if job_details.processed_dataset else "N/A"

    hyperparams_str = "\n".join([f"- **{k}**: {v}" for k, v in (job_details.hyperparameters or {}).items()]) or "N/A"
    metrics_str = "\n".join([f"- **{k}**: {v:.4f}" if isinstance(v, float) else f"- **{k}**: {v}" for k, v in (job_details.metrics or {}).items()]) or "N/A"
    
    base_model_id = (job_details.training_script_config or {}).get("base_model_id", "N/A")
    target_hf_repo_id = (job_details.training_script_config or {}).get("target_hf_repo_id", "YOUR_REPO_ID_MISSING_IN_CONFIG")

    # Construct ModelCardData for more structured metadata
    card_data = ModelCardData(
        language="en", # Or make configurable
        license=(job_details.training_script_config or {}).get("license", "mit"),
        library_name=(job_details.training_script_config or {}).get("library_name", "transformers"), # e.g., transformers, timm
        tags=[
            "autogenerated",
            job_details.model_type.lower().replace("_", "-") if job_details.model_type else "model",
            job_details.platform.value.lower(),
            "ai-training-platform" # Generic tag for your platform
        ],
        # pipeline_tag can be inferred or specified in training_script_config
        pipeline_tag=(job_details.training_script_config or {}).get("pipeline_tag") or \
                     (job_details.model_type.lower().replace("_", "-") if job_details.model_type else "unknown"),
        model_name=job_details.job_name,
        # Add more fields as needed: datasets, metrics, etc.
    )
    
    # You can add more structured data directly to card_data
    # For instance, if you have specific evaluation results in a standard format.
    # card_data.eval_results = [EvalResult(...)]

    md_content = f"""
        ---
        {card_data.to_yaml()}
        ---

        # Model Card for {job_details.job_name} (Job ID: {job_details.id})

        This model was trained as part of the AI Training Job: `{job_details.job_name}` using the `{job_details.platform.value}` platform.

        ## Model Details

        - **Model Type:** `{job_details.model_type or "N/A"}`
        - **Base Model (if fine-tuned):** `{base_model_id}`
        - **Source Job ID:** `{job_details.id}`
        - **Hugging Face Repo ID:** `{target_hf_repo_id}`

        ## Training Data

        - **Processed Dataset Name:** `{dataset_name}`
        - **Processed Dataset Source (Partial):** `{dataset_storage_url}`
        (Note: This might be an internal S3/GCS/Walrus URL, not necessarily a public dataset link.)

        ## Training Procedure

        ### Hyperparameters

        {hyperparams_str}

        ### Evaluation Results

        {metrics_str}

        ## How to use (Example - adjust based on actual model type)

        ```python
        # This is a generic example. Update based on your model's specifics.
        from transformers import AutoModelForSequenceClassification, AutoTokenizer # Or AutoModelForCausalLM, etc.

        model_id = "{target_hf_repo_id}"
        # Ensure tokenizer and model class match your actual model type
        # tokenizer = AutoTokenizer.from_pretrained(model_id)
        # model = AutoModelForSequenceClassification.from_pretrained(model_id)

        # Add a more specific usage example here based on job_details.model_type
        # For example:
        # inputs = tokenizer("Example text to classify.", return_tensors="pt")
        # outputs = model(**inputs)
        # logits = outputs.logits
        # predicted_class_id = logits.argmax().item()
        # print(f"Predicted class: {model.config.id2label[predicted_class_id]}")
        Use code with caution.
        Python
        Limitations and Bias
        {(job_details.training_script_config or {}).get("model_limitations_and_bias", "Information about model limitations and potential biases should be provided here.")}
        Disclaimer
        This model was trained automatically. Please evaluate its performance and biases carefully before use in production.
        """
    return md_content


async def _download_s3_artifact(
    artifact_url: str,
    local_target_path: str,
    aws_access_key_id: Optional[str],
    aws_secret_access_key: Optional[str],
    aws_region: str
):
    logger.info(f"Downloading from S3: {artifact_url} to {local_target_path}")
    parsed_url = urlparse(artifact_url)
    bucket_name = parsed_url.netloc
    key = parsed_url.path.lstrip('/')
    session = aiobotocore.session.get_session()
    # Configure credentials. If None, aiobotocore will try environment variables, IAM roles, etc.
    async with session.create_client(
        's3',
        region_name=aws_region,
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key
    ) as s3_client:
        try:
            response = await s3_client.get_object(Bucket=bucket_name, Key=key)
            async with aiofiles.open(local_target_path, "wb") as f:
                async for chunk in response['Body']:
                    await f.write(chunk)
            logger.info(f"S3 artifact downloaded successfully to {local_target_path}")
        except Exception as e:
            logger.error(f"Failed to download from S3 {artifact_url}: {e}", exc_info=True)
            raise


async def _download_gcs_artifact(
    artifact_url: str,
    local_target_path: str,
    gcp_project_id: Optional[str],
    gcp_credentials_path: Optional[str] # Path to service account JSON
):
    logger.info(f"Downloading from GCS: {artifact_url} to {local_target_path}")
    parsed_url = urlparse(artifact_url)
    bucket_name = parsed_url.netloc
    blob_name = parsed_url.path.lstrip('/')
    try:
        if gcp_credentials_path:
            credentials = service_account.Credentials.from_service_account_file(gcp_credentials_path)
            storage_client = storage.Client(project=gcp_project_id or credentials.project_id, credentials=credentials)
        else: # Rely on Application Default Credentials (ADC)
            storage_client = storage.Client(project=gcp_project_id)

        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)

        # google-cloud-storage doesn't have a native async download_to_filename yet.
        # Run sync_to_async or in a thread pool.
        # For simplicity here, we'll use a blocking call within an executor.
        # In a high-concurrency scenario, a proper thread pool executor is better.
        # loop = asyncio.get_event_loop()
        # await loop.run_in_executor(None, blob.download_to_filename, local_target_path)

        # For a simple approach for now (can be blocking):
        blob.download_to_filename(local_target_path) # This is blocking
        # To make it non-blocking, you'd wrap this in asyncio.to_thread (Python 3.9+)
        # import asyncio
        # await asyncio.to_thread(blob.download_to_filename, local_target_path)

        logger.info(f"GCS artifact downloaded successfully to {local_target_path}")
    except Exception as e:
        logger.error(f"Failed to download from GCS {artifact_url}: {e}", exc_info=True)
        raise


async def download_and_extract_artifacts(
    job_id: str, # For logging/temp dir naming
    artifact_url: str,
    artifact_storage_type: StorageType,
    target_local_dir: str, # Directory where contents should be extracted
    aws_access_key_id: Optional[str] = None,
    aws_secret_access_key: Optional[str] = None,
    aws_region: Optional[str] = None, # Fallback to AWS_REGION_MLOPS if None
    gcp_project_id: Optional[str] = None, # Fallback to GCP_PROJECT_ID_MLOPS
    gcp_credentials_path: Optional[str] = None # Fallback to GCP_SERVICE_ACCOUNT_KEY_PATH_MLOPS
) -> bool:
    """
    Downloads artifacts from cloud storage or copies local files, then extracts if it's an archive.
    The final model files will be directly inside target_local_dir.
    Returns True if successful, False otherwise.
    """
    await aios.makedirs(target_local_dir, exist_ok=True)
    parsed_url = urlparse(artifact_url)
    # Use a unique name for the downloaded archive to avoid conflicts if target_local_dir is reused.
    # The archive will be placed in a temporary sub-directory of target_local_dir or a system temp dir.
    temp_archive_holder = tempfile.mkdtemp(prefix=f"artifact_dl_{job_id}_")

    file_name = os.path.basename(parsed_url.path) if parsed_url.path else f"downloaded_artifact_{job_id}"
    local_archive_path = os.path.join(temp_archive_holder, file_name)

    downloaded = False

    try:
        if artifact_storage_type == StorageType.S3:
            _aws_region = aws_region or AWS_REGION_MLOPS
            await _download_s3_artifact(artifact_url, local_archive_path, aws_access_key_id, aws_secret_access_key, _aws_region)
            downloaded = True
        elif artifact_storage_type == StorageType.GCS:
            _gcp_project_id = gcp_project_id or GCP_PROJECT_ID_MLOPS
            _gcp_creds_path = gcp_credentials_path or GCP_SERVICE_ACCOUNT_KEY_PATH_MLOPS
            # Note: _download_gcs_artifact is currently blocking; for true async, use asyncio.to_thread
            import asyncio 
            await asyncio.to_thread( # Making the GCS download non-blocking
                _download_gcs_artifact,
                artifact_url, local_archive_path, _gcp_project_id, _gcp_creds_path
            )
            downloaded = True
        elif artifact_storage_type == StorageType.LOCAL_FS: # URL is a local path
            source_path = Path(artifact_url) # Assuming artifact_url is the direct path
            if not await aios.path.exists(source_path):
                logger.error(f"Local artifact URL does not exist: {source_path}")
                return False
            
            if await aios.path.isdir(source_path):
                logger.info(f"Source is a local directory {source_path}. Copying its content to {target_local_dir}")
                # shutil.copytree is sync. Use a custom async copy or run in thread.
                # For simplicity, let's use shutil.copytree in a thread.
                import asyncio
                def _copy_dir_sync():
                    if os.path.exists(target_local_dir): # Ensure target_local_dir is empty or handle content merge
                        for item in os.listdir(target_local_dir): # Simple clear, adjust if merge needed
                            item_path = os.path.join(target_local_dir, item)
                            if os.path.isdir(item_path): shutil.rmtree(item_path)
                            else: os.remove(item_path)

                    shutil.copytree(str(source_path), str(target_local_dir), dirs_exist_ok=True)
                await asyncio.to_thread(_copy_dir_sync)
                return True # Content is directly in target_local_dir, no extraction needed from archive path

            elif await aios.path.isfile(source_path):
                logger.info(f"Copying local file {source_path} to {local_archive_path}")
                await aiofiles.os.makedirs(os.path.dirname(local_archive_path), exist_ok=True)
                async with aiofiles.open(source_path, "rb") as src_f:
                    async with aiofiles.open(local_archive_path, "wb") as dst_f:
                        await dst_f.write(await src_f.read())
                downloaded = True
            else:
                logger.error(f"Local artifact path {source_path} is neither a file nor a directory.")
                return False
        else:
            # Handle Akave, Azure Blob, etc. here or raise error
            logger.error(f"Unsupported artifact storage type for download: {artifact_storage_type}")
            return False

        # Extraction logic (if a file was downloaded/copied to local_archive_path)
        if downloaded and await aios.path.exists(local_archive_path):
            logger.info(f"Attempting to extract {local_archive_path} into {target_local_dir}")
            # tarfile and zipfile are synchronous. Run in a thread pool for async.
            import asyncio
            def _extract_sync():
                if tarfile.is_tarfile(local_archive_path):
                    with tarfile.open(local_archive_path, "r:*") as tar:
                        tar.extractall(path=target_local_dir)
                    logger.info(f"Extracted tarball to {target_local_dir}")
                elif zipfile.is_zipfile(local_archive_path):
                    with zipfile.ZipFile(local_archive_path, "r") as zip_ref:
                        zip_ref.extractall(target_local_dir)
                    logger.info(f"Extracted zip file to {target_local_dir}")
                else:
                    # Not an archive, so copy the file itself to target_local_dir
                    # if target_local_dir is not the same as its parent.
                    final_dest = Path(target_local_dir) / Path(local_archive_path).name
                    if Path(local_archive_path).resolve() != final_dest.resolve():
                        shutil.copy2(local_archive_path, final_dest)
                        logger.info(f"Copied non-archive file {local_archive_path} to {final_dest}")
                    else:
                        logger.info(f"File {local_archive_path} is not an archive and is already in target dir structure. No extraction/copy needed.")
            
            await asyncio.to_thread(_extract_sync)
            return True
        elif not downloaded and artifact_storage_type != StorageType.LOCAL_FS: # only if LOCAL_FS dir copy handled it
            logger.error(f"Artifact download failed or local_archive_path {local_archive_path} not found after download attempt.")
            return False
        
        # If it was a LOCAL_FS directory copy, downloaded is False, but it was successful.
        # This case is handled by the early return in LOCAL_FS dir handling.
        # If it's a non-archive file that was downloaded, it's handled above.

        return True # Should be covered by logic above

    except Exception as e:
        logger.error(f"Error downloading/extracting artifacts from {artifact_url} for job {job_id}: {e}", exc_info=True)
        return False
    finally:
        # Clean up the temporary archive holder directory
        if await aios.path.exists(temp_archive_holder):
            try:
                # shutil.rmtree is sync, run in thread
                import asyncio
                await asyncio.to_thread(shutil.rmtree, temp_archive_holder)
                logger.info(f"Cleaned up temporary archive holder: {temp_archive_holder}")
            except Exception as e_clean:
                logger.warning(f"Could not clean up temp archive holder {temp_archive_holder}: {e_clean}")


def upload_to_huggingface( # Removed async
    local_model_dir: str,
    hf_repo_id: str,
    hf_token: str,
    job_details: AITrainingJob,
    commit_message: Optional[str] = None,
    private_repo: bool = False,
    generate_readme_if_missing: bool = True,
    hf_repo_type: str = "model"
) -> Optional[str]:
    """
    Uploads files from a local directory to a Hugging Face Hub repository synchronously.
    """
    try:
        logger.info(f"Initializing HfApi for repo: {hf_repo_id}")
        hf_api = HfApi(token=hf_token) # Changed from AsyncHfApi

        logger.info(f"Ensuring repository '{hf_repo_id}' exists (type: {hf_repo_type}, private: {private_repo})...")
        try:
            hf_api.create_repo( # Removed await
                repo_id=hf_repo_id,
                token=hf_token,
                private=private_repo,
                repo_type=hf_repo_type,
                exist_ok=True
            )
        except HfHubHTTPError as e:
            if e.response.status_code == 409: # Conflict, repo already exists
                logger.info(f"Repository {hf_repo_id} already exists.")
            else:
                raise # Re-raise other HF API errors

        if generate_readme_if_missing and hf_repo_type == "model":
            readme_path = Path(local_model_dir) / "README.md"
            # Changed from await aios.path.exists to os.path.exists
            if not os.path.exists(readme_path):
                logger.info(f"Generating model card for job {job_details.id} as README.md does not exist.")
                card_content = _generate_model_card_content_from_job(job_details)
                # Changed from async with aiofiles.open to with open, and await f.write to f.write
                with open(readme_path, "w", encoding="utf-8") as f:
                    f.write(card_content)
                logger.info(f"Model card generated and saved to {readme_path}")
            else:
                logger.info(f"README.md already exists in {local_model_dir}, not overwriting.")

        _commit_message = commit_message or f"Upload model artifacts for AI Training Platform Job ID: {job_details.id} ({job_details.job_name})"
        
        logger.info(f"Uploading contents of {local_model_dir} to {hf_repo_id} with commit: '{_commit_message}'")
        
        repo_url = hf_api.upload_folder( # Removed await, changed from async_hf_api
            folder_path=local_model_dir,
            repo_id=hf_repo_id,
            repo_type=hf_repo_type,
            commit_message=_commit_message,
        )
        
        logger.info(f"Successfully uploaded to Hugging Face Hub. Repo URL: {repo_url}")
        return repo_url

    except Exception as e:
        logger.error(f"Failed to upload to Hugging Face Hub repository {hf_repo_id} for job {job_details.id}: {e}", exc_info=True)
        return None